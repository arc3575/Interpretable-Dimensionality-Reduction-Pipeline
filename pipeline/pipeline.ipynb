{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyOJpfgd7-NS",
        "outputId": "e3488073-f451-4420-feb6-903b4b8efa4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#To connect to my drive, in order to access datasets (.csv form)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You may need to install mygene\n",
        "#It's used to look up ensembl names for genes and reference them with their known names\n",
        "#Some genes have been measured, but are 'Uncharacterized', making them of high intrigue since they may point to unknown underlying biological phenomena\n",
        "!pip install mygene"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rARUTwsT8x2b",
        "outputId": "74e1c738-1bc3-4e2d-e9ff-8d32b0c2399c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting biothings-client>=0.2.6 (from mygene)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from biothings-client>=0.2.6->mygene) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (4.14.1)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biothings-client, mygene\n",
            "Successfully installed biothings-client-0.4.1 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cudf\n",
        "\n",
        "from cuml.decomposition import IncrementalPCA as cuIncPCA\n",
        "from sklearn.decomposition import SparsePCA, PCA, KernelPCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import cupy as cp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from scipy.stats import skew\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "#Look up table to translate ensembl gene code into gene name\n",
        "#Might need to install mygene, uncomment below\n",
        "#!pip install mygene\n",
        "import mygene"
      ],
      "metadata": {
        "id": "4Ah7IzdX8EKf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This list holds the format for the hyperparameter tuning module in the hybrid_pipeline\n",
        "CLASSIFIER_TUNING_COLUMNS = [\n",
        "    \"Combo Rank\", \"PCA_Type\", \"PC_Num\", \"Classifier\",\n",
        "    \"n_estimators (rf)\", \"max_depth (rf)\",\n",
        "    \"C (logreg)\",\n",
        "    \"C (svm)\", \"gamma (svm)\",\n",
        "    \"learning_rate (xgb)\", \"max_depth (xgb)\",\n",
        "    \"F1_Score\"\n",
        "]"
      ],
      "metadata": {
        "id": "6Pt8-vnJ8ENL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This pipeline only processes one .csv file at a time\n",
        "#It is designed to handle large datasets, but only one dataset at a time\n",
        "#Change the csv_path below to the desired path\n",
        "\n",
        "# csv_path = \"/content/drive/MyDrive/TCGA-BRCA_samples_labeled.csv\"\n",
        "csv_path = \"/content/drive/MyDrive/TCGA-BRCA_small.csv\""
      ],
      "metadata": {
        "id": "9Oq6y9Id8EP4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master Controller"
      ],
      "metadata": {
        "id": "Uy0-0Z8XyKuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the controller class that controls the following modules\n",
        "#1. Correlation module\n",
        "#2. Hybrid module\n",
        "#3. Interpretable Module\n",
        "#The master controller is how the user interacts with implementation\n",
        "class MasterPipeline:\n",
        "    def __init__(self, csv_path, label_col='label'):\n",
        "        \"\"\"\n",
        "        Orchestrates CorrelationPipeline, HybridPCAPipeline, and InterpretablePCAPipeline.\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.label_col = label_col\n",
        "        self.corr_pipeline = CorrelationPipeline(csv_path, label_col=label_col)\n",
        "        self.hybrid_pipeline = HybridPCAPipeline(csv_path, label_col=label_col)\n",
        "        self.interp_pipeline = None\n",
        "        self.revolver_scores = None\n",
        "\n",
        "    #Stage 1: Run Correlation Pipeline\n",
        "    def run_correlation_analysis(self, corr_threshold=0.5, log_transform=True):\n",
        "        self.corr_pipeline.extract_high_corr_features(threshold=corr_threshold)\n",
        "        return self.corr_pipeline.high_corr_features\n",
        "\n",
        "    #Stage 2: Run Hybrid PCA + LDA\n",
        "    def run_hybrid_pipeline(self, pca_components=1000, final_components=100, model_num=3, lda_components=[2,5,10,20,50,100]):\n",
        "        self.hybrid_pipeline.pca_components = pca_components\n",
        "        self.hybrid_pipeline.final_components = final_components\n",
        "\n",
        "        self.hybrid_pipeline.load_data()\n",
        "        self.hybrid_pipeline.run_stage1_pca()\n",
        "        self.hybrid_pipeline.run_stage2_pca_tuning(model_num=model_num)\n",
        "        self.hybrid_pipeline.run_classifier_tuning()\n",
        "        self.hybrid_pipeline.get_optimal_classifier_parameters()\n",
        "        self.hybrid_pipeline.cross_validate_optimal_combos()\n",
        "        self.hybrid_pipeline.run_lda_tuning(component_list=lda_components)\n",
        "\n",
        "        # Build interpretable pipeline\n",
        "        self.interp_pipeline = InterpretablePCAPipeline(self.hybrid_pipeline, top_n_genes=10000)\n",
        "        self.interp_pipeline.extract_pca_loadings()\n",
        "        self.interp_pipeline.extract_lda_loadings()\n",
        "        return self.interp_pipeline\n",
        "\n",
        "    #Revolver Module results\n",
        "    def save_revolver_scores(self, filepath=\"revolver_f1_scores.csv\"):\n",
        "        if self.hybrid_pipeline.revolver_eval_df is None:\n",
        "            raise ValueError(\"Run run_hybrid_pipeline() first.\")\n",
        "        results_df = self.hybrid_pipeline.revolver_eval_df\n",
        "        self.revolver_scores = results_df\n",
        "        results_df.to_csv(filepath, index=False)\n",
        "        print(f\"Revolver scores saved to {filepath}\")\n",
        "        return results_df\n",
        "\n",
        "    def get_top_hybrid_results(self, n=10, sort_by=\"F1_Score\"):\n",
        "        return self.hybrid_pipeline.get_top_combinations(n=n, sort_by=sort_by)\n",
        "\n",
        "    #Interpretability, top genes\n",
        "    #This is a static method, but I placed it in the master controller for organization sake\n",
        "    @staticmethod\n",
        "    def map_ensembl_to_gene(ensembl_ids):\n",
        "        \"\"\"\n",
        "        Maps a list of Ensembl IDs to gene symbols and biotypes using mygene.\n",
        "        \"\"\"\n",
        "        import mygene\n",
        "        mg = mygene.MyGeneInfo()\n",
        "        results = mg.querymany(ensembl_ids, scopes='ensembl.gene', fields='symbol,type_of_gene', species='human')\n",
        "        mapping = {}\n",
        "        for r in results:\n",
        "            mapping[r['query']] = {\n",
        "                'symbol': r.get('symbol', 'N/A'),\n",
        "                'biotype': r.get('type_of_gene', 'N/A')\n",
        "            }\n",
        "        return mapping\n",
        "    #Get top genes for both LDA and PCA\n",
        "    #Through the mygene library these genes need to be converted from their ensembl names to their characterized names\n",
        "    def get_top_genes(self, n=10):\n",
        "        \"\"\"\n",
        "        Returns a combined table of top N genes from PC1 and LD1.\n",
        "        \"\"\"\n",
        "        pc1_df = self.interp_pipeline.get_top_genes_pc1(n)\n",
        "        ld1_df = self.interp_pipeline.get_top_genes_ld1(n)\n",
        "        return pd.concat([pc1_df, ld1_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "b25xcm4m8EVn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Module"
      ],
      "metadata": {
        "id": "AW7ePi7JyHpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the Correlation module\n",
        "#It is used to find the correlation coefficients with the target variable\n",
        "#The normal threshold is currently set at 0.5, but it can be set to any number.\n",
        "#The pre-processing code should give insights into what threshold should be chosen\n",
        "#Anything above this threshold (postive and negative) is considered highly correlated\n",
        "class CorrelationPipeline:\n",
        "    def __init__(self, csv_path, label_col='label', normal_threshold=0.5, log_transform=True):\n",
        "        self.csv_path = csv_path\n",
        "        self.label_col = label_col\n",
        "        self.normal_threshold = normal_threshold\n",
        "\n",
        "        print(\"Loading dataset...\")\n",
        "        self.df = pd.read_csv(self.csv_path).drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "        self.labels = self.df[self.label_col]\n",
        "        self.df = self.df.drop(columns=[self.label_col])\n",
        "\n",
        "        if log_transform:\n",
        "            print(\"Applying log2 transform to features...\")\n",
        "            self.df = np.log2(self.df + 1)\n",
        "\n",
        "        self.high_corr_features = None\n",
        "        self.df_corr = None\n",
        "\n",
        "    #Skewness Distribution\n",
        "    def feature_skewness_summary(self):\n",
        "        skewness = self.df.apply(skew, nan_policy='omit')\n",
        "        summary = pd.DataFrame({\n",
        "            \"Distribution Type\": [\"Normal\", \"Left-Skewed\", \"Right-Skewed\"],\n",
        "            \"Count\": [\n",
        "                (skewness.abs() < self.normal_threshold).sum(),\n",
        "                (skewness < -self.normal_threshold).sum(),\n",
        "                (skewness > self.normal_threshold).sum()\n",
        "            ]\n",
        "        })\n",
        "        return summary\n",
        "\n",
        "    def sample_skewness_summary(self, log_transform=False):\n",
        "        data = np.log2(self.df + 1) if log_transform else self.df.to_numpy()\n",
        "        skewness = skew(data, axis=1, nan_policy='omit')\n",
        "        summary = pd.DataFrame({\n",
        "            \"Distribution Type\": [\"Normal\", \"Left-Skewed\", \"Right-Skewed\"],\n",
        "            \"Count\": [\n",
        "                (np.abs(skewness) < self.normal_threshold).sum(),\n",
        "                (skewness < -self.normal_threshold).sum(),\n",
        "                (skewness > self.normal_threshold).sum()\n",
        "            ]\n",
        "        })\n",
        "        return summary\n",
        "\n",
        "    def plot_sample_skewness(self, log_transform=False, bins=30, title=\"Sample Skewness Distribution\"):\n",
        "\n",
        "        data = np.log2(self.df + 1) if log_transform else self.df.to_numpy()\n",
        "        skewness = skew(data, axis=1, nan_policy='omit')  # skewness per sample\n",
        "\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.hist(skewness, bins=bins, color='purple', edgecolor='black', alpha=0.7)\n",
        "        plt.axvline(0, color='red', linestyle='--', label='Symmetric')\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Skewness\")\n",
        "        plt.ylabel(\"Number of Samples\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_pie(self, summary, title=\"Distribution Types\"):\n",
        "        plt.figure(figsize=(6,6))\n",
        "        plt.pie(summary['Count'], labels=summary['Distribution Type'], autopct='%1.1f%%',\n",
        "                startangle=140, wedgeprops={'edgecolor':'black'})\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    #Extract Feature Correlation\n",
        "    def feature_label_correlation_summary(self, step=0.1):\n",
        "        corr = self.df.corrwith(pd.Series(self.labels))\n",
        "        bins = np.arange(-1, 1 + step, step)\n",
        "        categories = pd.cut(corr, bins=bins, right=True, include_lowest=True)\n",
        "        counts = categories.value_counts().sort_index()\n",
        "        summary = pd.DataFrame({\n",
        "            \"Correlation Range\": counts.index.astype(str),\n",
        "            \"Count\": counts.values\n",
        "        })\n",
        "        return summary\n",
        "\n",
        "    def plot_correlation_histogram(self, summary, title=\"Feature-Label Correlation Histogram\"):\n",
        "        plt.figure(figsize=(12,6))\n",
        "        plt.bar(summary['Correlation Range'], summary['Count'], color='skyblue', edgecolor='black')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.xlabel(\"Correlation Range with Target Variable\")\n",
        "        plt.ylabel(\"Number of Features\")\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def extract_high_corr_features(self, threshold=0.5):\n",
        "        corr = self.df.corrwith(pd.Series(self.labels))\n",
        "        self.high_corr_features = corr[abs(corr) > threshold].index.tolist()\n",
        "        self.df_corr = self.df[self.high_corr_features]\n",
        "        print(f\"Selected {len(self.high_corr_features)} features with |correlation| > {threshold}\")\n",
        "        return self.df_corr\n",
        "\n",
        "    #Classifiers with correlation\n",
        "    def corr_benchmark(self, use_pca=False, n_components=20):\n",
        "        X = self.df_corr if self.df_corr is not None else self.df\n",
        "        y = self.labels\n",
        "\n",
        "        if use_pca:\n",
        "            pca = PCA(n_components=n_components)\n",
        "            X = pca.fit_transform(X)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        classifiers = {\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "            'SVM': SVC(probability=True, random_state=42),\n",
        "            'Voting': VotingClassifier(\n",
        "                estimators=[\n",
        "                    ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
        "                    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "                    ('svm', SVC(probability=True, random_state=42))\n",
        "                ],\n",
        "                voting='soft'\n",
        "            )\n",
        "        }\n",
        "        results = {}\n",
        "        for name, clf in classifiers.items():\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            results[name] = f1_score(y_test, y_pred)\n",
        "        return pd.DataFrame(list(results.items()), columns=['Classifier', 'F1-Score'])\n",
        "\n",
        "    def plot_pca_separation(self, n_components=2, use_high_corr=True, cmap='coolwarm'):\n",
        "\n",
        "        # Select data\n",
        "        X = self.df_corr if (use_high_corr and self.df_corr is not None) else self.df\n",
        "        y = self.labels\n",
        "\n",
        "        # Run PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        plt.figure(figsize=(8,6))\n",
        "\n",
        "        if n_components == 2:\n",
        "            plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=cmap, alpha=0.7, edgecolor='k')\n",
        "            plt.xlabel(\"PC1\")\n",
        "            plt.ylabel(\"PC2\")\n",
        "            plt.title(\"PCA (2D) of Features\")\n",
        "        elif n_components == 3:\n",
        "            from mpl_toolkits.mplot3d import Axes3D\n",
        "            ax = plt.figure(figsize=(8,6)).add_subplot(111, projection='3d')\n",
        "            ax.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=y, cmap=cmap, alpha=0.7)\n",
        "            ax.set_xlabel(\"PC1\")\n",
        "            ax.set_ylabel(\"PC2\")\n",
        "            ax.set_zlabel(\"PC3\")\n",
        "            ax.set_title(\"PCA (3D) of Features\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    #Cross validation\n",
        "    def cross_val_classifiers(self, n_splits=5):\n",
        "        X = self.df_corr if self.df_corr is not None else self.df\n",
        "        y = self.labels\n",
        "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        classifiers = {\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "            'SVM': SVC(probability=True, random_state=42),\n",
        "            'Voting': VotingClassifier(\n",
        "                estimators=[\n",
        "                    ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
        "                    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "                    ('svm', SVC(probability=True, random_state=42))\n",
        "                ],\n",
        "                voting='soft'\n",
        "            )\n",
        "        }\n",
        "        results = []\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, X, y, cv=cv, scoring='f1')\n",
        "            results.append({'Classifier': name, 'Mean F1': round(scores.mean(), 4), 'Std F1': round(scores.std(), 4)})\n",
        "        return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "N-bIetyv8Qr9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Module (Revolver)"
      ],
      "metadata": {
        "id": "8Exc24d2yDrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the largest module\n",
        "#The hybrid module contains the revolver submodule (PCA + classifier combinations)\n",
        "#It also contains the GPU-based incremental PCA transformation stage\n",
        "#The hybrid model can be broken up into three stages\n",
        "#Stage 1 = GPU-Based Incremental PCA (You can change pca_components based on dataset size)\n",
        "#Stage 2 = CPU-Based Revolver submodule\n",
        "#Stage 3 = Hyperparameter Tuning\n",
        "class HybridPCAPipeline:\n",
        "    def __init__(self, csv_path, label_col=\"label\",\n",
        "                 pca_components=1000, final_components=100,\n",
        "                 batch_size=512, alpha=1.0, logreg_max_iter=5000):\n",
        "        self.csv_path = csv_path\n",
        "        self.label_col = label_col\n",
        "        self.pca_components = pca_components\n",
        "        self.final_components = final_components\n",
        "        self.batch_size = batch_size if pca_components <= batch_size else pca_components\n",
        "        self.logreg_max_iter = logreg_max_iter\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Data\n",
        "        self.df = None\n",
        "        self.X_gpu = None\n",
        "        self.y_cpu = None\n",
        "        self.gene_names = None\n",
        "\n",
        "        # Stage 1\n",
        "        self.intermediate_pca = None\n",
        "        self.X_pca_cpu = None\n",
        "\n",
        "        # Stage 2\n",
        "        self.revolver_results = None          # raw transformed PCA outputs\n",
        "        self.revolver_eval_df = None          # evaluated PCA+classifier scores\n",
        "        self.top_combos = None\n",
        "        self.pc_tuning_df = None\n",
        "        self.optimal_pc_nums = None\n",
        "        self.reduced_datasets = None\n",
        "\n",
        "        # Stage 3\n",
        "        self.classifier_tuning_df = None\n",
        "\n",
        "    #Loading in the Data\n",
        "    def load_data(self, log_transform=True, scale=True):\n",
        "        self.df = cudf.read_csv(self.csv_path)\n",
        "        if 'Unnamed: 0' in self.df.columns:\n",
        "            self.df = self.df.drop(columns=['Unnamed: 0'])\n",
        "        X_cpu = self.df.drop(columns=[self.label_col]).to_pandas().astype('float32')\n",
        "        if log_transform:\n",
        "            X_cpu = np.log1p(X_cpu)\n",
        "        if scale:\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            scaler = StandardScaler()\n",
        "            X_cpu = scaler.fit_transform(X_cpu)\n",
        "        self.X_gpu = cudf.DataFrame.from_records(X_cpu)\n",
        "        self.y_cpu = self.df[self.label_col].to_numpy()\n",
        "        self.gene_names = list(self.df.drop(columns=[self.label_col]).columns)\n",
        "        print(f\"Data loaded: {self.X_gpu.shape[0]} samples, {self.X_gpu.shape[1]} features.\")\n",
        "\n",
        "    #Stage 1: Incremental PCA\n",
        "    def run_stage1_pca(self):\n",
        "        print(\"Running GPU Incremental PCA...\")\n",
        "        start = time.time()\n",
        "        self.intermediate_pca = cuIncPCA(n_components=self.pca_components, batch_size=self.batch_size)\n",
        "        X_pca_gpu = self.intermediate_pca.fit_transform(self.X_gpu)\n",
        "        self.X_pca_cpu = X_pca_gpu.to_cupy().get()\n",
        "        print(f\"Incremental PCA completed in {time.time() - start:.2f} sec.\")\n",
        "        return self.X_pca_cpu\n",
        "\n",
        "    def plot_stage1_cumulative_variance(self):\n",
        "        if self.intermediate_pca is None:\n",
        "            raise ValueError(\"Run run_stage1_pca() first.\")\n",
        "        var_ratio = self.intermediate_pca.explained_variance_ratio_.to_numpy()\n",
        "        cumulative_var = np.cumsum(var_ratio)\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.plot(range(1, len(cumulative_var)+1), cumulative_var, marker='o')\n",
        "        plt.xlabel(\"Number of Components\")\n",
        "        plt.ylabel(\"Cumulative Explained Variance\")\n",
        "        plt.title(\"Stage 1 Incremental PCA: Cumulative Explained Variance\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        return cumulative_var\n",
        "\n",
        "    #Stage 2: Revolver submodule & Evaluation\n",
        "    def revolver(self, X_pca_cpu, n_components=None, pca_subset=None):\n",
        "        if n_components is None:\n",
        "            n_components = self.final_components\n",
        "        pca_methods = {\n",
        "            \"SparsePCA\": SparsePCA(n_components=n_components, alpha=self.alpha, random_state=42, n_jobs=-1),\n",
        "            \"RotationPCA\": PCA(n_components=n_components, svd_solver='full'),\n",
        "            \"KernelPCA_rbf\": KernelPCA(n_components=n_components, kernel='rbf'),\n",
        "            \"KernelPCA_poly\": KernelPCA(n_components=n_components, kernel='poly'),\n",
        "            \"KernelPCA_sigmoid\": KernelPCA(n_components=n_components, kernel='sigmoid')\n",
        "        }\n",
        "        if pca_subset:\n",
        "            if pca_subset not in pca_methods:\n",
        "                raise ValueError(f\"PCA subset {pca_subset} not recognized.\")\n",
        "            pca_methods = {pca_subset: pca_methods[pca_subset]}\n",
        "        results = {}\n",
        "        for name, model in pca_methods.items():\n",
        "            start = time.time()\n",
        "            X_reduced = model.fit_transform(X_pca_cpu)\n",
        "            results[name] = np.asarray(X_reduced, dtype=np.float32)\n",
        "            print(f\"{name} completed in {time.time() - start:.2f} sec.\")\n",
        "        return results if not pca_subset else results[pca_subset]\n",
        "\n",
        "    def evaluate_classifiers(self, revolver_results):\n",
        "        classifiers = {\n",
        "            \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "            \"LogisticRegression\": LogisticRegression(max_iter=self.logreg_max_iter),\n",
        "            \"SVM\": make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True)),\n",
        "            \"XGBoost\": XGBClassifier(\n",
        "                n_estimators=300,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=6,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                eval_metric='logloss',\n",
        "                random_state=42\n",
        "            )\n",
        "        }\n",
        "        results = []\n",
        "        for pca_name, X_reduced in revolver_results.items():\n",
        "            for clf_name, clf in classifiers.items():\n",
        "                score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                results.append((pca_name, clf_name, score))\n",
        "        results_df = pd.DataFrame(results, columns=[\"PCA_Type\", \"Classifier\", \"F1_Score\"])\n",
        "        results_df = results_df.sort_values(by=\"F1_Score\", ascending=False).reset_index(drop=True)\n",
        "        return results_df\n",
        "\n",
        "    def _get_classifier(self, clf_name):\n",
        "        if clf_name == \"RandomForest\":\n",
        "            return RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "        elif clf_name == \"LogisticRegression\":\n",
        "            return LogisticRegression(max_iter=self.logreg_max_iter)\n",
        "        elif clf_name == \"SVM\":\n",
        "            return make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True))\n",
        "        elif clf_name == \"XGBoost\":\n",
        "            return XGBClassifier(\n",
        "                n_estimators=300,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=6,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                eval_metric='logloss',\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown classifier: {clf_name}\")\n",
        "\n",
        "    def stage2_pc_tuning(self, X_pca_cpu, top_combos, component_list=[2,10,50,100,200,500,800]):\n",
        "        tuning_results = []\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']\n",
        "            clf_name = row['Classifier']\n",
        "            for n in component_list:\n",
        "                X_reduced = self.revolver(X_pca_cpu, n_components=n, pca_subset=pca_type)\n",
        "                clf = self._get_classifier(clf_name)\n",
        "                score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                tuning_results.append((combo_rank, pca_type, clf_name, n, score))\n",
        "        return pd.DataFrame(tuning_results, columns=[\"Combo Rank\", \"PCA_Type\", \"Classifier\", \"Num_Components\", \"F1_Score\"])\n",
        "\n",
        "    def get_optimal_pc_nums(self, pc_tuning_df):\n",
        "        optimal_params = {}\n",
        "        for (combo_rank, pca_type, clf_name), group in pc_tuning_df.groupby(['Combo Rank', 'PCA_Type', 'Classifier']):\n",
        "            best_row = group.loc[group['F1_Score'].idxmax()]\n",
        "            optimal_params[(combo_rank, pca_type, clf_name)] = int(best_row['Num_Components'])\n",
        "        return optimal_params\n",
        "\n",
        "    def run_stage2_pca_tuning(self, model_num=3, component_list=[2,10,50,100,200,500,800]):\n",
        "        print(\"\\nRunning Stage 2 PCA Tuning...\")\n",
        "        self.revolver_results = self.revolver(self.X_pca_cpu)\n",
        "        self.revolver_eval_df = self.evaluate_classifiers(self.revolver_results)\n",
        "        self.top_combos = self.revolver_eval_df.head(model_num)\n",
        "        self.pc_tuning_df = self.stage2_pc_tuning(self.X_pca_cpu, self.top_combos, component_list)\n",
        "        self.optimal_pc_nums = self.get_optimal_pc_nums(self.pc_tuning_df)\n",
        "        print(\"Stage 2 PCA tuning complete.\")\n",
        "        return self.pc_tuning_df\n",
        "\n",
        "    def get_revolver_results(self):\n",
        "        if hasattr(self, 'revolver_eval_df') and self.revolver_eval_df is not None:\n",
        "            return self.revolver_eval_df\n",
        "        else:\n",
        "            raise AttributeError(\"Revolver evaluation results not available. Run run_stage2_pca_tuning() first.\")\n",
        "\n",
        "    def get_top_combinations(self, n=10, sort_by=\"F1_Score\"):\n",
        "        df = self.get_revolver_results()\n",
        "        return df.sort_values(by=sort_by, ascending=False).head(n)\n",
        "\n",
        "\n",
        "    #Stage 3: Classifier Tuning\n",
        "    def rf_tuning(self, reduced_datasets, top_combos, n_estimators_list=[100,200,500], max_depth_list=[5,10,None]):\n",
        "        tuning_results = []\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']; clf_name = row['Classifier']\n",
        "            if clf_name != \"RandomForest\": continue\n",
        "            pc_num = self.optimal_pc_nums[(combo_rank, pca_type, clf_name)]\n",
        "            X_reduced = reduced_datasets[(combo_rank, pca_type, clf_name)]\n",
        "            for n_est in n_estimators_list:\n",
        "                for depth in max_depth_list:\n",
        "                    clf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, random_state=42)\n",
        "                    score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                    tuning_results.append([combo_rank,pca_type,pc_num,clf_name,n_est,depth,0,0,0,0,0,score])\n",
        "        return pd.DataFrame(tuning_results, columns=[\n",
        "            \"Combo Rank\",\"PCA_Type\",\"Num_Components\",\"Classifier\",\n",
        "            \"n_estimators (rf)\",\"max_depth (rf)\",\"C (logreg)\",\"C (svm)\",\"gamma (svm)\",\n",
        "            \"learning_rate (xgb)\",\"max_depth (xgb)\",\"F1_Score\"\n",
        "        ])\n",
        "\n",
        "    def logreg_tuning(self, reduced_datasets, top_combos, C_list=[0.01,0.1,1,10,100]):\n",
        "        tuning_results = []\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']; clf_name = row['Classifier']\n",
        "            if clf_name != \"LogisticRegression\": continue\n",
        "            pc_num = self.optimal_pc_nums[(combo_rank, pca_type, clf_name)]\n",
        "            X_reduced = reduced_datasets[(combo_rank, pca_type, clf_name)]\n",
        "            for C in C_list:\n",
        "                clf = LogisticRegression(C=C, max_iter=self.logreg_max_iter)\n",
        "                score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                tuning_results.append([combo_rank,pca_type,pc_num,clf_name,0,0,C,0,0,0,0,score])\n",
        "        return pd.DataFrame(tuning_results, columns=[\n",
        "            \"Combo Rank\",\"PCA_Type\",\"Num_Components\",\"Classifier\",\n",
        "            \"n_estimators (rf)\",\"max_depth (rf)\",\"C (logreg)\",\"C (svm)\",\"gamma (svm)\",\n",
        "            \"learning_rate (xgb)\",\"max_depth (xgb)\",\"F1_Score\"\n",
        "        ])\n",
        "\n",
        "    def svm_tuning(self, reduced_datasets, top_combos, C_list=[0.1,1,10], gamma_list=['scale','auto']):\n",
        "        tuning_results = []\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']; clf_name = row['Classifier']\n",
        "            if clf_name != \"SVM\": continue\n",
        "            pc_num = self.optimal_pc_nums[(combo_rank, pca_type, clf_name)]\n",
        "            X_reduced = reduced_datasets[(combo_rank, pca_type, clf_name)]\n",
        "            for C in C_list:\n",
        "                for gamma in gamma_list:\n",
        "                    clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=C, gamma=gamma, probability=True))\n",
        "                    score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                    tuning_results.append([combo_rank,pca_type,pc_num,clf_name,0,0,0,C,gamma,0,0,score])\n",
        "        return pd.DataFrame(tuning_results, columns=[\n",
        "            \"Combo Rank\",\"PCA_Type\",\"Num_Components\",\"Classifier\",\n",
        "            \"n_estimators (rf)\",\"max_depth (rf)\",\"C (logreg)\",\"C (svm)\",\"gamma (svm)\",\n",
        "            \"learning_rate (xgb)\",\"max_depth (xgb)\",\"F1_Score\"\n",
        "        ])\n",
        "\n",
        "    def xgb_tuning(self, reduced_datasets, top_combos, learning_rates=[0.01,0.05,0.1], max_depths=[3,6,10]):\n",
        "        tuning_results = []\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']; clf_name = row['Classifier']\n",
        "            if clf_name != \"XGBoost\": continue\n",
        "            pc_num = self.optimal_pc_nums[(combo_rank, pca_type, clf_name)]\n",
        "            X_reduced = reduced_datasets[(combo_rank, pca_type, clf_name)]\n",
        "            for lr in learning_rates:\n",
        "                for depth in max_depths:\n",
        "                    clf = XGBClassifier(n_estimators=300,learning_rate=lr,max_depth=depth,\n",
        "                                        subsample=0.8,colsample_bytree=0.8,\n",
        "                                        eval_metric='logloss',random_state=42)\n",
        "                    score = cross_val_score(clf, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "                    tuning_results.append([combo_rank,pca_type,pc_num,clf_name,0,0,0,0,0,lr,depth,score])\n",
        "        return pd.DataFrame(tuning_results, columns=[\n",
        "            \"Combo Rank\",\"PCA_Type\",\"Num_Components\",\"Classifier\",\n",
        "            \"n_estimators (rf)\",\"max_depth (rf)\",\"C (logreg)\",\"C (svm)\",\"gamma (svm)\",\n",
        "            \"learning_rate (xgb)\",\"max_depth (xgb)\",\"F1_Score\"\n",
        "        ])\n",
        "\n",
        "    def run_classifier_tuning(self):\n",
        "        print(\"\\nRunning Classifier Tuning...\")\n",
        "        # Build reduced datasets with optimal component counts\n",
        "        self.reduced_datasets = self.get_stage2_reduced_datasets(self.X_pca_cpu, self.top_combos)\n",
        "        all_results = []\n",
        "        all_results.append(self.rf_tuning(self.reduced_datasets, self.top_combos))\n",
        "        all_results.append(self.logreg_tuning(self.reduced_datasets, self.top_combos))\n",
        "        all_results.append(self.svm_tuning(self.reduced_datasets, self.top_combos))\n",
        "        all_results.append(self.xgb_tuning(self.reduced_datasets, self.top_combos))\n",
        "        self.classifier_tuning_df = pd.concat(all_results, ignore_index=True)\n",
        "        print(\"Classifier tuning complete.\")\n",
        "        return self.classifier_tuning_df\n",
        "\n",
        "    def get_stage2_reduced_datasets(self, X_pca_cpu, top_combos):\n",
        "        reduced_data = {}\n",
        "        for combo_rank, (_, row) in enumerate(top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']\n",
        "            clf_name = row['Classifier']\n",
        "            optimal_components = self.optimal_pc_nums[(combo_rank, pca_type, clf_name)]\n",
        "            X_reduced = self.revolver(X_pca_cpu, n_components=optimal_components, pca_subset=pca_type)\n",
        "            reduced_data[(combo_rank, pca_type, clf_name)] = X_reduced\n",
        "        return reduced_data\n",
        "\n",
        "    def get_optimal_classifier_parameters(self):\n",
        "\n",
        "        if self.classifier_tuning_df is None:\n",
        "            raise ValueError(\"Run run_classifier_tuning() first.\")\n",
        "\n",
        "        optimal_params = {}\n",
        "        for (combo_rank, pca_type, clf_name), group in self.classifier_tuning_df.groupby(\n",
        "            [\"Combo Rank\", \"PCA_Type\", \"Classifier\"]\n",
        "        ):\n",
        "            best_row = group.loc[group['F1_Score'].idxmax()]\n",
        "            params = {\n",
        "                \"n_estimators\": int(best_row.get(\"n_estimators (rf)\", 0)),\n",
        "                \"max_depth_rf\": int(best_row.get(\"max_depth (rf)\", 0)) if not pd.isna(best_row.get(\"max_depth (rf)\", 0)) else None,\n",
        "                \"C_logreg\": float(best_row.get(\"C (logreg)\", 0)),\n",
        "                \"C_svm\": float(best_row.get(\"C (svm)\", 0)),\n",
        "                \"gamma_svm\": best_row.get(\"gamma (svm)\", 'scale'),\n",
        "                \"learning_rate_xgb\": float(best_row.get(\"learning_rate (xgb)\", 0)),\n",
        "                \"max_depth_xgb\": int(best_row.get(\"max_depth (xgb)\", 0))\n",
        "            }\n",
        "            optimal_params[(combo_rank, pca_type, clf_name)] = params\n",
        "\n",
        "        self.optimal_classifier_params = optimal_params\n",
        "        print(\"Optimal classifier parameters stored.\")\n",
        "        return optimal_params\n",
        "\n",
        "    def cross_validate_optimal_combos(self, cv=5):\n",
        "\n",
        "        if not hasattr(self, 'optimal_classifier_params'):\n",
        "            raise ValueError(\"Run get_optimal_classifier_parameters() first.\")\n",
        "\n",
        "        results = []\n",
        "        for (combo_rank, pca_type, clf_name), params in self.optimal_classifier_params.items():\n",
        "            X_reduced = self.reduced_datasets[(combo_rank, pca_type, clf_name)]\n",
        "\n",
        "            # Build classifier with optimal params\n",
        "            if clf_name == \"RandomForest\":\n",
        "                clf = RandomForestClassifier(\n",
        "                    n_estimators=params['n_estimators'] or 200,\n",
        "                    max_depth=params['max_depth_rf'] if params['max_depth_rf'] > 0 else None,\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif clf_name == \"LogisticRegression\":\n",
        "                clf = make_pipeline(\n",
        "                    StandardScaler(),\n",
        "                    LogisticRegression(C=params['C_logreg'] or 1.0, max_iter=self.logreg_max_iter)\n",
        "                )\n",
        "            elif clf_name == \"SVM\":\n",
        "                clf = make_pipeline(\n",
        "                    StandardScaler(),\n",
        "                    SVC(C=params['C_svm'] or 1.0, gamma=params['gamma_svm'] or 'scale', kernel='rbf', probability=True)\n",
        "                )\n",
        "            elif clf_name == \"XGBoost\":\n",
        "                clf = XGBClassifier(\n",
        "                    n_estimators=300,\n",
        "                    learning_rate=params['learning_rate_xgb'] or 0.05,\n",
        "                    max_depth=params['max_depth_xgb'] or 6,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    eval_metric='logloss',\n",
        "                    random_state=42\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown classifier: {clf_name}\")\n",
        "\n",
        "            score = cross_val_score(clf, X_reduced, self.y_cpu, cv=cv, scoring='f1').mean()\n",
        "            results.append((combo_rank, pca_type, clf_name, score))\n",
        "\n",
        "        final_results = pd.DataFrame(results, columns=[\"Combo Rank\", \"PCA_Type\", \"Classifier\", \"F1_Score\"])\n",
        "        print(\"Cross-validation with optimal parameters complete.\")\n",
        "        return final_results\n",
        "\n",
        "    def lda_tuning(self, X_pca_cpu, component_list=[2,5,10,20,50,100]):\n",
        "        tuning_results = []\n",
        "        for n in component_list:\n",
        "            if n >= X_pca_cpu.shape[1]:\n",
        "                continue  # skip if asking for more components than available\n",
        "            X_reduced = X_pca_cpu[:, :n]  # take first n PCs\n",
        "            lda = LinearDiscriminantAnalysis()\n",
        "            score = cross_val_score(lda, X_reduced, self.y_cpu, cv=5, scoring='f1').mean()\n",
        "            tuning_results.append((n, score))\n",
        "        df = pd.DataFrame(tuning_results, columns=[\"Num_Components\", \"F1_Score\"])\n",
        "        best_row = df.loc[df['F1_Score'].idxmax()]\n",
        "        self.best_lda = {\n",
        "            \"Num_Components\": int(best_row[\"Num_Components\"]),\n",
        "            \"F1_Score\": float(best_row[\"F1_Score\"])\n",
        "        }\n",
        "        print(f\"Best LDA: {self.best_lda['Num_Components']} components (F1={self.best_lda['F1_Score']:.4f})\")\n",
        "        return df\n",
        "\n",
        "    def run_lda_tuning(self, component_list=[2,5,10,20,50,100]):\n",
        "        print(\"\\nRunning LDA tuning...\")\n",
        "        if self.X_pca_cpu is None:\n",
        "            raise ValueError(\"Run run_stage1_pca() first.\")\n",
        "        self.lda_tuning_df = self.lda_tuning(self.X_pca_cpu, component_list)\n",
        "        print(\"LDA tuning complete.\")\n",
        "        return self.lda_tuning_df"
      ],
      "metadata": {
        "id": "rjWlDG0A8QvC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretable Module"
      ],
      "metadata": {
        "id": "qD_O89GBx8xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the final module, the interpretable module\n",
        "#This module is back calculates the loadings of the revolver PCA through chaining with the incremental PCA df\n",
        "#It gets loadings from linear discriminant analysis\n",
        "#It compares genes between\n",
        "#1. Revolver-based principal components\n",
        "#2. Linear discriminant analysis (Note: Number of linear discriminants is limited by the number of classes, minus one)\n",
        "#3. Highly-Correlated Genes (Module #1)\n",
        "class InterpretablePCAPipeline:\n",
        "    def __init__(self, hybrid_pipeline, top_n_genes=10000):\n",
        "        self.hybrid = hybrid_pipeline\n",
        "        self.top_n_genes = top_n_genes\n",
        "        self.gene_names = self.hybrid.gene_names\n",
        "        self.top_combos = self._filter_combos(self.hybrid.top_combos)\n",
        "        self.optimal_pc_nums = self.hybrid.optimal_pc_nums\n",
        "        self.optimal_clf_params = self.hybrid.optimal_classifier_params\n",
        "        self.best_lda = getattr(self.hybrid, 'best_lda', None)\n",
        "\n",
        "        # Convert Stage 1 loadings (Incremental PCA) to NumPy\n",
        "        stage1_components = self.hybrid.intermediate_pca.components_\n",
        "        if isinstance(stage1_components, cudf.DataFrame):\n",
        "            self.stage1_loadings = stage1_components.to_pandas().to_numpy()\n",
        "        elif hasattr(stage1_components, \"__cuda_array_interface__\"):  # cupy array\n",
        "            self.stage1_loadings = cp.asnumpy(stage1_components)\n",
        "        else:\n",
        "            self.stage1_loadings = stage1_components\n",
        "\n",
        "        # Storage\n",
        "        self.pca_model = None\n",
        "        self.lda_model = None\n",
        "        self.loadings_dict = {}\n",
        "        self.full_component_loadings = {}\n",
        "        self.full_lda_loadings = {}\n",
        "        self.lda_full_vector = None\n",
        "\n",
        "    def _filter_combos(self, top_combos):\n",
        "        interpretable = top_combos[~top_combos['PCA_Type'].str.startswith(\"KernelPCA\")]\n",
        "        if not interpretable.empty:\n",
        "            return interpretable\n",
        "        print(\"All top combos are KernelPCA. Substituting with next best interpretable combos...\")\n",
        "        all_results = self.hybrid.evaluate_classifiers(self.hybrid.revolver_results)\n",
        "        interpretable_alts = all_results[~all_results['PCA_Type'].str.startswith(\"KernelPCA\")]\n",
        "        return interpretable_alts.head(len(top_combos))\n",
        "\n",
        "    # PCA Loadings\n",
        "    def extract_pca_loadings(self):\n",
        "\n",
        "        for combo_rank, (_, row) in enumerate(self.top_combos.iterrows(), start=1):\n",
        "            pca_type = row['PCA_Type']\n",
        "            clf_name = row['Classifier']\n",
        "            n_components = self.optimal_pc_nums.get((combo_rank, pca_type, clf_name), self.hybrid.final_components)\n",
        "\n",
        "            if pca_type.startswith(\"KernelPCA\"):\n",
        "                print(f\"Skipping {pca_type}: No interpretable loadings.\")\n",
        "                continue\n",
        "\n",
        "            if pca_type == \"SparsePCA\":\n",
        "                stage2_pca = SparsePCA(n_components=n_components, alpha=self.hybrid.alpha, random_state=42, n_jobs=-1)\n",
        "            elif pca_type == \"RotationPCA\":\n",
        "                stage2_pca = PCA(n_components=n_components, svd_solver='full')\n",
        "\n",
        "            stage2_pca.fit(self.hybrid.X_pca_cpu)\n",
        "            self.pca_model = stage2_pca\n",
        "            stage2_loadings = stage2_pca.components_\n",
        "            combined_loadings = np.dot(stage2_loadings, self.stage1_loadings)\n",
        "            for i in range(n_components):\n",
        "                comp_name = f\"PC{i+1}\"\n",
        "                self.full_component_loadings[comp_name] = pd.Series(combined_loadings[i, :], index=self.gene_names)\n",
        "\n",
        "    #LDA Loadings\n",
        "    def extract_lda_loadings(self):\n",
        "\n",
        "        if not self.best_lda:\n",
        "            raise ValueError(\"Run pipeline.run_lda_tuning() first.\")\n",
        "        n_components = self.best_lda['Num_Components']\n",
        "        X_reduced = self.hybrid.X_pca_cpu[:, :n_components]\n",
        "        lda = LinearDiscriminantAnalysis(solver='svd')\n",
        "        lda.fit(X_reduced, self.hybrid.y_cpu)\n",
        "        self.lda_model = lda\n",
        "        lda_loadings = lda.scalings_\n",
        "        combined_lda_loadings = np.dot(self.stage1_loadings[:n_components, :].T, lda_loadings)\n",
        "        for j in range(combined_lda_loadings.shape[1]):\n",
        "            disc_name = f\"LD{j+1}\"\n",
        "            self.full_lda_loadings[disc_name] = pd.Series(combined_lda_loadings[:, j], index=self.gene_names)\n",
        "        self.lda_full_vector = self.full_lda_loadings[\"LD1\"]\n",
        "\n",
        "    #Top Gene Extraction\n",
        "    def get_top_genes_pc1(self, n=10):\n",
        "        if not self.pca_model:\n",
        "            raise AttributeError(\"PCA model not found. Run extract_pca_loadings() first.\")\n",
        "        loadings = self.pca_model.components_[0]\n",
        "        gene_ids = self.gene_names\n",
        "        abs_loadings = np.abs(loadings)\n",
        "        top_idx = np.argsort(abs_loadings)[::-1][:n]\n",
        "        top_genes = [(i+1, loadings[idx], 'PC1', gene_ids[idx]) for i, idx in enumerate(top_idx)]\n",
        "        mapping = MasterPipeline.map_ensembl_to_gene([g for _, _, _, g in top_genes])\n",
        "        return pd.DataFrame(\n",
        "            [[rank, loading, axis, ensembl_id, mapping.get(ensembl_id, {}).get('symbol', 'N/A'), mapping.get(ensembl_id, {}).get('biotype', 'N/A')]\n",
        "             for rank, loading, axis, ensembl_id in top_genes],\n",
        "            columns=['Rank', 'Loading', 'Axis Type', 'Ensembl Code', 'Gene Name', 'Biotype']\n",
        "        )\n",
        "\n",
        "    def get_top_genes_ld1(self, n=10):\n",
        "        if not self.lda_model:\n",
        "            raise AttributeError(\"LDA model not found. Run extract_lda_loadings() first.\")\n",
        "        loadings = self.lda_model.coef_[0]\n",
        "        gene_ids = self.gene_names\n",
        "        abs_loadings = np.abs(loadings)\n",
        "        top_idx = np.argsort(abs_loadings)[::-1][:n]\n",
        "        top_genes = [(i+1, loadings[idx], 'LD1', gene_ids[idx]) for i, idx in enumerate(top_idx)]\n",
        "        mapping = MasterPipeline.map_ensembl_to_gene([g for _, _, _, g in top_genes])\n",
        "        return pd.DataFrame(\n",
        "            [[rank, loading, axis, ensembl_id, mapping.get(ensembl_id, {}).get('symbol', 'N/A'), mapping.get(ensembl_id, {}).get('biotype', 'N/A')]\n",
        "             for rank, loading, axis, ensembl_id in top_genes],\n",
        "            columns=['Rank', 'Loading', 'Axis Type', 'Ensembl Code', 'Gene Name', 'Biotype']\n",
        "        )\n",
        "\n",
        "    #PCA-LDA & PCA-PCA Comparisons\n",
        "    def compare_pca_lda_matrix(self, max_pcs=10, max_lds=10):\n",
        "        from scipy.stats import spearmanr\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        results = []\n",
        "        pcs = list(self.full_component_loadings.keys())[:max_pcs]\n",
        "        lds = list(self.full_lda_loadings.keys())[:max_lds]\n",
        "        for pc_name in pcs:\n",
        "            for ld_name in lds:\n",
        "                pc_vec = self.full_component_loadings[pc_name]\n",
        "                ld_vec = self.full_lda_loadings[ld_name]\n",
        "                common = pc_vec.index.intersection(ld_vec.index)\n",
        "                pc_aligned, ld_aligned = pc_vec.loc[common], ld_vec.loc[common]\n",
        "                spearman_corr, _ = spearmanr(pc_aligned, ld_aligned)\n",
        "                cos_sim = cosine_similarity(pc_aligned.values.reshape(1,-1), ld_aligned.values.reshape(1,-1))[0,0]\n",
        "                pc_top = set(pc_aligned.abs().nlargest(self.top_n_genes).index)\n",
        "                ld_top = set(ld_aligned.abs().nlargest(self.top_n_genes).index)\n",
        "                jaccard = len(pc_top & ld_top) / len(pc_top | ld_top) if (pc_top | ld_top) else 0\n",
        "                results.append({\"PC\": pc_name, \"LD\": ld_name, \"Jaccard_topN\": jaccard, \"Spearman_all\": spearman_corr, \"Cosine_all\": cos_sim})\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def compare_pcs_matrix(self, max_pcs=25):\n",
        "        from itertools import combinations\n",
        "        from scipy.stats import spearmanr\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        results = []\n",
        "        pcs = list(self.full_component_loadings.keys())[:max_pcs]\n",
        "        for pc1, pc2 in combinations(pcs, 2):\n",
        "            v1 = self.full_component_loadings[pc1]\n",
        "            v2 = self.full_component_loadings[pc2]\n",
        "            common = v1.index.intersection(v2.index)\n",
        "            v1, v2 = v1.loc[common], v2.loc[common]\n",
        "            spearman_corr, _ = spearmanr(v1, v2)\n",
        "            cos_sim = cosine_similarity(v1.values.reshape(1,-1), v2.values.reshape(1,-1))[0,0]\n",
        "            top1 = set(v1.abs().nlargest(self.top_n_genes).index)\n",
        "            top2 = set(v2.abs().nlargest(self.top_n_genes).index)\n",
        "            jaccard = len(top1 & top2) / len(top1 | top2) if (top1 | top2) else 0\n",
        "            results.append({\"PC1\": pc1, \"PC2\": pc2, \"Jaccard_topN\": jaccard, \"Spearman_all\": spearman_corr, \"Cosine_all\": cos_sim})\n",
        "        return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "_LT11HNd8QxZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation Code"
      ],
      "metadata": {
        "id": "wXkjW6nWx4bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize MasterPipeline\n",
        "master = MasterPipeline(csv_path, label_col='label')\n",
        "\n",
        "# Step 2: Run the full hybrid pipeline (Stages 1–3 inside)\n",
        "master.run_hybrid_pipeline(\n",
        "    pca_components=1000,       # Stage 1: Incremental PCA\n",
        "    final_components=100,      # Stage 2: PCA variants\n",
        "    model_num=3,               # Top PCA-classifier combos\n",
        "    lda_components=[2,5,10,20,50,100]  # LDA tuning\n",
        ")\n",
        "\n",
        "# Step 3: Get top 10 PCA-classifier combinations\n",
        "top_results = master.get_top_hybrid_results(n=10)\n",
        "print(top_results)\n",
        "\n",
        "# Step 4: Get top 5 genes from PC1 and LD1\n",
        "top_genes = master.get_top_genes(n=5)\n",
        "print(top_genes.to_latex(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3HoGJzyA0Kb",
        "outputId": "c1671eca-ad75-450c-b246-f2bdde56d2f6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Applying log2 transform to features...\n",
            "Data loaded: 1219 samples, 1213 features.\n",
            "Running GPU Incremental PCA...\n",
            "Incremental PCA completed in 0.50 sec.\n",
            "\n",
            "Running Stage 2 PCA Tuning...\n",
            "SparsePCA completed in 1.04 sec.\n",
            "RotationPCA completed in 0.42 sec.\n",
            "KernelPCA_rbf completed in 0.36 sec.\n",
            "KernelPCA_poly completed in 0.36 sec.\n",
            "KernelPCA_sigmoid completed in 0.39 sec.\n",
            "RotationPCA completed in 0.41 sec.\n",
            "RotationPCA completed in 1.26 sec.\n",
            "RotationPCA completed in 0.59 sec.\n",
            "RotationPCA completed in 1.63 sec.\n",
            "RotationPCA completed in 0.64 sec.\n",
            "RotationPCA completed in 0.49 sec.\n",
            "RotationPCA completed in 1.37 sec.\n",
            "SparsePCA completed in 0.78 sec.\n",
            "SparsePCA completed in 0.92 sec.\n",
            "SparsePCA completed in 1.26 sec.\n",
            "SparsePCA completed in 1.81 sec.\n",
            "SparsePCA completed in 2.23 sec.\n",
            "SparsePCA completed in 1.85 sec.\n",
            "SparsePCA completed in 4.81 sec.\n",
            "RotationPCA completed in 1.03 sec.\n",
            "RotationPCA completed in 0.37 sec.\n",
            "RotationPCA completed in 0.35 sec.\n",
            "RotationPCA completed in 0.37 sec.\n",
            "RotationPCA completed in 0.37 sec.\n",
            "RotationPCA completed in 0.35 sec.\n",
            "RotationPCA completed in 0.37 sec.\n",
            "Stage 2 PCA tuning complete.\n",
            "\n",
            "Running Classifier Tuning...\n",
            "RotationPCA completed in 0.36 sec.\n",
            "SparsePCA completed in 1.44 sec.\n",
            "RotationPCA completed in 0.46 sec.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4080303578.py:265: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.classifier_tuning_df = pd.concat(all_results, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier tuning complete.\n",
            "Optimal classifier parameters stored.\n",
            "Cross-validation with optimal parameters complete.\n",
            "\n",
            "Running LDA tuning...\n",
            "Best LDA: 100 components (F1=0.9959)\n",
            "LDA tuning complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "INFO:biothings.client:querying 1-5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            PCA_Type          Classifier  F1_Score\n",
            "0        RotationPCA  LogisticRegression  0.995469\n",
            "1          SparsePCA  LogisticRegression  0.995015\n",
            "2        RotationPCA             XGBoost  0.994601\n",
            "3          SparsePCA             XGBoost  0.994601\n",
            "4  KernelPCA_sigmoid             XGBoost  0.994123\n",
            "5     KernelPCA_poly  LogisticRegression  0.993686\n",
            "6      KernelPCA_rbf                 SVM  0.993246\n",
            "7      KernelPCA_rbf             XGBoost  0.993218\n",
            "8     KernelPCA_poly                 SVM  0.992772\n",
            "9     KernelPCA_poly             XGBoost  0.992327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:biothings.client:Finished.\n",
            "WARNING:biothings.client:5 input query terms found no hit:\t['ENSG00000177868.12', 'ENSG00000127529.7', 'ENSG00000079805.18', 'ENSG00000275314.1', 'ENSG00000283\n",
            "INFO:biothings.client:Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "WARNING:biothings.client:Input sequence provided is already in string format. No operation performed\n",
            "INFO:biothings.client:querying 1-5 ...\n",
            "INFO:biothings.client:Finished.\n",
            "WARNING:biothings.client:5 input query terms found no hit:\t['ENSG00000221533.1', 'ENSG00000182722.5', 'ENSG00000105011.9', 'ENSG00000235834.1', 'ENSG0000028011\n",
            "INFO:biothings.client:Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{rrllll}\n",
            "\\toprule\n",
            "Rank & Loading & Axis Type & Ensembl Code & Gene Name & Biotype \\\\\n",
            "\\midrule\n",
            "1 & 1.000000 & PC1 & ENSG00000177868.12 & N/A & N/A \\\\\n",
            "2 & -0.000000 & PC1 & ENSG00000127529.7 & N/A & N/A \\\\\n",
            "3 & 0.000000 & PC1 & ENSG00000079805.18 & N/A & N/A \\\\\n",
            "4 & -0.000000 & PC1 & ENSG00000275314.1 & N/A & N/A \\\\\n",
            "5 & -0.000000 & PC1 & ENSG00000283674.3 & N/A & N/A \\\\\n",
            "1 & 1.941333 & LD1 & ENSG00000221533.1 & N/A & N/A \\\\\n",
            "2 & -1.360556 & LD1 & ENSG00000182722.5 & N/A & N/A \\\\\n",
            "3 & -1.064643 & LD1 & ENSG00000105011.9 & N/A & N/A \\\\\n",
            "4 & 1.006867 & LD1 & ENSG00000235834.1 & N/A & N/A \\\\\n",
            "5 & -0.922339 & LD1 & ENSG00000280115.1 & N/A & N/A \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}